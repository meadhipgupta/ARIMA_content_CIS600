{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54cc8a8b",
   "metadata": {},
   "source": [
    "## This code is to use ML methods for Solar Irradiance Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commonly used python functions and display settings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # specify to ignore warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa6a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key imports for this code (various ML and Stat Models)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "import pmdarima as pm\n",
    "from pmdarima import model_selection\n",
    "from pmdarima import auto_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import viz libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "from plotly.graph_objs import *\n",
    "from plotly import tools\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import pyplot\n",
    "from pandas.plotting import autocorrelation_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5bf964",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c184d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files\n",
    "all_types_weekly_df = pd.read_csv(\"../Datasets/avocado.csv\", parse_dates=[\"Date\"])\n",
    "temp_df = pd.read_csv(\"../Datasets/temperature.csv\", parse_dates=[\"datetime\"])\n",
    "weekly_df = all_types_weekly_df[all_types_weekly_df['type'] == 'conventional']\n",
    "weekly_df[\"region\"] = weekly_df[\"region\"].replace(\"SanDiego\", \"San Diego\")\n",
    "\n",
    "# check how the deaders look\n",
    "weekly_df.head()\n",
    "temp_df.head()\n",
    "\n",
    "# Filter weekly.csv for the specified cities\n",
    "cities = [\"Atlanta\", \"Boston\", \"Houston\", \"San Diego\"]\n",
    "weekly_df = weekly_df[weekly_df[\"region\"].isin(cities)].copy()\n",
    "\n",
    "# Convert datetime to date-only format in temp_df\n",
    "temp_df[\"Date\"] = pd.to_datetime(temp_df[\"datetime\"]).dt.normalize()\n",
    "\n",
    "# Remove weekly entries beyond available temperature data \n",
    "latest_temp_date = temp_df[\"Date\"].max()\n",
    "weekly_df = weekly_df[weekly_df[\"Date\"] <= latest_temp_date]\n",
    "\n",
    "# Initialize a list to collect results\n",
    "merged_data = []\n",
    "\n",
    "# Process each row in weekly_df\n",
    "for _, row in weekly_df.iterrows():\n",
    "    city = row[\"region\"]  # Get the specific city for the row\n",
    "    start_date = row[\"Date\"]\n",
    "    end_date = start_date + pd.Timedelta(days=6)\n",
    "\n",
    "    # Compute weekly statistics for the specific city\n",
    "    if city in temp_df.columns:\n",
    "        city_weekly_temps = temp_df[(temp_df[\"Date\"] >= start_date) & \n",
    "                                    (temp_df[\"Date\"] <= end_date)][city].dropna()\n",
    "        temp_stats = {\n",
    "            \"min_temp\": city_weekly_temps.min(),\n",
    "            \"max_temp\": city_weekly_temps.max(),\n",
    "            \"avg_temp\": city_weekly_temps.mean(),\n",
    "            \"stdev_temp\": city_weekly_temps.std(),\n",
    "        }\n",
    "    else:\n",
    "        temp_stats = {\"min_temp\": None, \"max_temp\": None, \"avg_temp\": None, \"stdev_temp\": None}\n",
    "\n",
    "    # Append row with computed statistics\n",
    "    merged_data.append({**row.to_dict(), **temp_stats})\n",
    "\n",
    "# Convert results to DataFrame and sort by region and date\n",
    "final_df = pd.DataFrame(merged_data).sort_values(by=[\"region\", \"Date\"]).reset_index(drop=True)\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe94576-d4b8-49fe-aec6-e108dd90563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a plot of Weekly Average Price of Avocado in Atlanta\n",
    "plot_data = []\n",
    "atl_df = final_df[final_df['region'] == 'Atlanta']\n",
    "plot_data.append(go.Scatter(x= final_df['Date'], y= atl_df['AveragePrice']))\n",
    "layout = go.Layout(xaxis = dict(title='Date'), yaxis = dict(title= 'Average Price of Avocado'), \n",
    "                   title = 'Time Series of Average Price of Avocado')\n",
    "fig = go.Figure(data= plot_data, layout=layout)\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2dbc3d-e14a-4a9e-a491-884cf118a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a plot of Total Volume of acocado in Atlanta\n",
    "plot_data = []\n",
    "atl_df = final_df[final_df['region'] == 'Atlanta']\n",
    "plot_data.append(go.Scatter(x= final_df['Date'], y= atl_df['Total Volume']))\n",
    "layout = go.Layout(xaxis = dict(title='Date'), yaxis = dict(title= 'Total Volumee of Avocado'), \n",
    "                   title = 'Time Series of Total Volume of Avocado')\n",
    "fig = go.Figure(data= plot_data, layout=layout)\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47a01c-8f9e-414a-89cf-bafe5c1063d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a graph of the autocorrelation function versus lags for the avocado\n",
    "sm.graphics.tsa.plot_acf(atl_df['AveragePrice'].values.squeeze(), lags=40)\n",
    "sm.graphics.tsa.plot_acf(atl_df['Total Volume'].values.squeeze(), lags=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b6369-da06-401e-98c9-4e293cef956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a graph of the partial autocorrelation function versus lags for the avocado\n",
    "sm.graphics.tsa.plot_pacf(atl_df['AveragePrice'].values.squeeze(), lags=40)\n",
    "sm.graphics.tsa.plot_pacf(atl_df['Total Volume'].values.squeeze(), lags=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a71780-4bc9-48bc-b0c0-518b833a38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we get the time series\n",
    "time_series = atl_df['AveragePrice']\n",
    "\n",
    "# Define the number of predictions to make \n",
    "h = 1\n",
    "\n",
    "# Define the length of each training set\n",
    "T = 100\n",
    "\n",
    "# Initialize the lists to store the percentage and absolute errors\n",
    "perc_error_list = []\n",
    "abs_error_list = []\n",
    "\n",
    "es_preds_train = np.zeros(T+h) # In case we wish to use the ES predictions\n",
    "\n",
    "# Loop through the data frame and make predictions using exponential smoothing\n",
    "for i in range(len(time_series) - T - h):\n",
    "    # Define the training and testing data sets\n",
    "    train = time_series.iloc[i:i+T].values\n",
    "    test = time_series.iloc[i+T:i+T+h].values\n",
    "    \n",
    "    # Fit the exponential smoothing model\n",
    "    model = ExponentialSmoothing(train, trend='add') \n",
    "    fit_model = model.fit()\n",
    "    \n",
    "    # Make predictions\n",
    "    pred_list = fit_model.forecast(h)\n",
    "    preds = pred_list[h-1]\n",
    "\n",
    "    # Calculate percentage and absolute errors\n",
    "    perc_errors = np.abs(test[h-1]-preds)/test[h-1]\n",
    "    abs_errors = np.abs(test[h-1]-preds)\n",
    "\n",
    "    # Store the percentage and absolute errors\n",
    "    perc_error_list.append(perc_errors)\n",
    "    abs_error_list.append(abs_errors)\n",
    "    \n",
    "    # Get the ES predictions\n",
    "    es_preds_train = np.append(es_preds_train, preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891766b-aa07-436c-91c1-bd273655996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage-error results\n",
    "print('Mean absolute percentage error:', np.mean(perc_error_list, axis = 0))\n",
    "print('Median absolute percentage error:', np.median(perc_error_list, axis = 0))\n",
    "print('75th percentile of absolute percentage error:', np.percentile(perc_error_list, 75, axis = 0))\n",
    "print('90th percentile of absolute percentage error:', np.percentile(perc_error_list, 90, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1f41d-53b8-41e8-b899-979c914b53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the absolute error ratio results\n",
    "avg_global = atl_df['AveragePrice'][T+h:].mean()\n",
    "print('Mean absolute error ratio:', np.mean(abs_error_list, axis = 0)/avg_global)\n",
    "print('Median absolute error ratio:', np.median(abs_error_list, axis = 0)/avg_global)\n",
    "print('75th percentile absolute error ratio:', np.percentile(abs_error_list, 75, axis = 0)/avg_global)\n",
    "print('90th percentile absolute error ratio:', np.percentile(abs_error_list, 90, axis = 0)/avg_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1f40c-338c-4e19-98dd-f2a0ce103013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we get the time series\n",
    "time_series = atl_df['Total Volume']\n",
    "\n",
    "# Define the number of predictions to make \n",
    "h = 1\n",
    "\n",
    "# Define the length of each training set\n",
    "T = 100\n",
    "\n",
    "# Initialize the lists to store the percentage and absolute errors\n",
    "perc_error_list = []\n",
    "abs_error_list = []\n",
    "\n",
    "es_preds_train = np.zeros(T+h) # In case we wish to use the ES predictions\n",
    "\n",
    "# Loop through the data frame and make predictions using exponential smoothing\n",
    "for i in range(len(time_series) - T - h):\n",
    "    # Define the training and testing data sets\n",
    "    train = time_series.iloc[i:i+T].values\n",
    "    test = time_series.iloc[i+T:i+T+h].values\n",
    "    \n",
    "    # Fit the exponential smoothing model\n",
    "    model = ExponentialSmoothing(train, trend='add') \n",
    "    fit_model = model.fit()\n",
    "    \n",
    "    # Make predictions\n",
    "    pred_list = fit_model.forecast(h)\n",
    "    preds = pred_list[h-1]\n",
    "\n",
    "    # Calculate percentage and absolute errors\n",
    "    perc_errors = np.abs(test[h-1]-preds)/test[h-1]\n",
    "    abs_errors = np.abs(test[h-1]-preds)\n",
    "\n",
    "    # Store the percentage and absolute errors\n",
    "    perc_error_list.append(perc_errors)\n",
    "    abs_error_list.append(abs_errors)\n",
    "    \n",
    "    # Get the ES predictions\n",
    "    es_preds_train = np.append(es_preds_train, preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362d075-59b4-42f3-923e-2651d5c0aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage-error results\n",
    "print('Mean absolute percentage error:', np.mean(perc_error_list, axis = 0))\n",
    "print('Median absolute percentage error:', np.median(perc_error_list, axis = 0))\n",
    "print('75th percentile of absolute percentage error:', np.percentile(perc_error_list, 75, axis = 0))\n",
    "print('90th percentile of absolute percentage error:', np.percentile(perc_error_list, 90, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a701409-1075-4271-bb9f-faace1f2ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the absolute error ratio results\n",
    "avg_global = atl_df['Total Volume'][T+h:].mean()\n",
    "print('Mean absolute error ratio:', np.mean(abs_error_list, axis = 0)/avg_global)\n",
    "print('Median absolute error ratio:', np.median(abs_error_list, axis = 0)/avg_global)\n",
    "print('75th percentile absolute error ratio:', np.percentile(abs_error_list, 75, axis = 0)/avg_global)\n",
    "print('90th percentile absolute error ratio:', np.percentile(abs_error_list, 90, axis = 0)/avg_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e3e4b-b172-4dc9-a780-6d0c99c7405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting optimal differencing \n",
    "d_opt = pm.arima.ndiffs(atl_df['AveragePrice'].iloc[0:T])\n",
    "d_opt\n",
    "d_opt = pm.arima.ndiffs(atl_df['Total Volume'].iloc[0:T])\n",
    "d_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228cc7cb-3081-45c3-b8a0-a1ecf1541a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we get the time series\n",
    "time_series = atl_df['AveragePrice']\n",
    "\n",
    "# Define the number of predictions to make \n",
    "h = 1\n",
    "\n",
    "# Define the length of each training set\n",
    "T = 100\n",
    "\n",
    "# Initialize the lists to store the percentage and absolute errors\n",
    "ar_perc_error_list = []\n",
    "ar_abs_error_list = []\n",
    "\n",
    "ar_preds_train = np.zeros(T+h) # In case we wish to use the ARIMA predictions\n",
    "\n",
    "# Loop through the data frame and make predictions using ARIMA\n",
    "for i in range(len(time_series) - T - h):\n",
    "    # Define the training and testing data sets\n",
    "    train = time_series.iloc[i:i+T].values\n",
    "    test = time_series.iloc[i+T:i+T+h].values\n",
    "\n",
    "    # Using a specified order (this would need to be fine-tuned)\n",
    "    order = (2, 0, 1) \n",
    "    # seasonal_order = (1, 0, 0, seasonal_periods) \n",
    "\n",
    "    # Fit the SARIMAX or ARIMA model\n",
    "    model = SARIMAX(endog=train, exog=None, order=order, seasonal_order=None)\n",
    "    fit_model = model.fit(disp=False)\n",
    "\n",
    "    # Make predictions\n",
    "    pred_list = fit_model.forecast(steps=len(test), exog=None)\n",
    "    preds = pred_list[h-1]\n",
    "\n",
    "    # Calculate percentage and absolute errors\n",
    "    ar_perc_errors = np.abs(test[h-1]-preds)/test[h-1]\n",
    "    ar_abs_errors = np.abs(test[h-1]-preds)\n",
    "\n",
    "    # Store the percentage and absolute errors\n",
    "    ar_perc_error_list.append(ar_perc_errors)\n",
    "    ar_abs_error_list.append(ar_abs_errors)\n",
    "    \n",
    "    # Get the ARIMA predictions\n",
    "    ar_preds_train = np.append(ar_preds_train, preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec62f05c-1b60-4784-801b-d08725977501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage-error results\n",
    "print('Mean absolute percentage error:', np.mean(ar_perc_error_list, axis = 0))\n",
    "print('Median absolute percentage error:', np.median(ar_perc_error_list, axis = 0))\n",
    "print('75th percentile of absolute percentage error:', np.percentile(ar_perc_error_list, 75, axis = 0))\n",
    "print('90th percentile of absolute percentage error:', np.percentile(ar_perc_error_list, 90, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353aed9-2b72-4bbf-990d-32291f4a1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the absolute error ratio results\n",
    "avg_global = atl_df['AveragePrice'][T+h:].mean()\n",
    "print('Mean absolute error ratio:', np.mean(ar_abs_error_list, axis = 0)/avg_global)\n",
    "print('Median absolute error ratio:', np.median(ar_abs_error_list, axis = 0)/avg_global)\n",
    "print('75th percentile absolute error ratio:', np.percentile(ar_abs_error_list, 75, axis = 0)/avg_global)\n",
    "print('90th percentile absolute error ratio:', np.percentile(ar_abs_error_list, 90, axis = 0)/avg_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa921dd-38c3-440d-8f28-08d63b38c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we get the time series\n",
    "time_series = atl_df['Total Volume']\n",
    "\n",
    "# Define the number of predictions to make \n",
    "h = 1\n",
    "\n",
    "# Define the length of each training set\n",
    "T = 100\n",
    "\n",
    "# Initialize the lists to store the percentage and absolute errors\n",
    "ar_perc_error_list = []\n",
    "ar_abs_error_list = []\n",
    "\n",
    "ar_preds_train = np.zeros(T+h) # In case we wish to use the ARIMA predictions\n",
    "\n",
    "# Loop through the data frame and make predictions using ARIMA\n",
    "for i in range(len(time_series) - T - h):\n",
    "    # Define the training and testing data sets\n",
    "    train = time_series.iloc[i:i+T].values\n",
    "    test = time_series.iloc[i+T:i+T+h].values\n",
    "\n",
    "    # Using a specified order (this would need to be fine-tuned)\n",
    "    order = (2, 1, 1) \n",
    "    # seasonal_order = (1, 0, 0, seasonal_periods) \n",
    "\n",
    "    # Fit the SARIMAX or ARIMA model\n",
    "    model = SARIMAX(endog=train, exog=None, order=order, seasonal_order=None)\n",
    "    fit_model = model.fit(disp=False)\n",
    "\n",
    "    # Make predictions\n",
    "    pred_list = fit_model.forecast(steps=len(test), exog=None)\n",
    "    preds = pred_list[h-1]\n",
    "\n",
    "    # Calculate percentage and absolute errors\n",
    "    ar_perc_errors = np.abs(test[h-1]-preds)/test[h-1]\n",
    "    ar_abs_errors = np.abs(test[h-1]-preds)\n",
    "\n",
    "    # Store the percentage and absolute errors\n",
    "    ar_perc_error_list.append(ar_perc_errors)\n",
    "    ar_abs_error_list.append(ar_abs_errors)\n",
    "    \n",
    "    # Get the ARIMA predictions\n",
    "    ar_preds_train = np.append(ar_preds_train, preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce980e-2558-4c5c-9c8e-b8c70c03311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage-error results\n",
    "print('Mean absolute percentage error:', np.mean(ar_perc_error_list, axis = 0))\n",
    "print('Median absolute percentage error:', np.median(ar_perc_error_list, axis = 0))\n",
    "print('75th percentile of absolute percentage error:', np.percentile(ar_perc_error_list, 75, axis = 0))\n",
    "print('90th percentile of absolute percentage error:', np.percentile(ar_perc_error_list, 90, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6a641-4e2f-4988-a839-fcc5d3dff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the absolute error ratio results\n",
    "avg_global = atl_df['Total Volume'][T+h:].mean()\n",
    "print('Mean absolute error ratio:', np.mean(ar_abs_error_list, axis = 0)/avg_global)\n",
    "print('Median absolute error ratio:', np.median(ar_abs_error_list, axis = 0)/avg_global)\n",
    "print('75th percentile absolute error ratio:', np.percentile(ar_abs_error_list, 75, axis = 0)/avg_global)\n",
    "print('90th percentile absolute error ratio:', np.percentile(ar_abs_error_list, 90, axis = 0)/avg_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319f31f-f435-41d7-b82c-2eacb37c5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lists to store the percentage and absolute errors\n",
    "ar_perc_error_list = []\n",
    "ar_abs_error_list = []\n",
    "\n",
    "ar_preds_train = np.zeros(T+h) # In case we wish to use the ARIMA predictions\n",
    "\n",
    "# Loop through the data frame and make predictions using ARIMA\n",
    "for i in range(len(time_series) - T - h):\n",
    "    # Define the training and testing data sets\n",
    "    train = time_series.iloc[i:i+T].values\n",
    "    test = time_series.iloc[i+T:i+T+h].values\n",
    "    exog_df = atl_df[['min_temp', 'max_temp', 'avg_temp', 'stdev_temp']].iloc[i:i+T].values\n",
    "    exog_test = atl_df[['min_temp', 'max_temp', 'avg_temp', 'stdev_temp']].iloc[i+T:i+T+h].values\n",
    "\n",
    "    # Using a specified order (this would need to be fine-tuned)\n",
    "    order = (2, 1, 1) \n",
    "    # seasonal_order = (1, 0, 0, seasonal_periods) \n",
    "\n",
    "    # Fit the SARIMAX or ARIMA model\n",
    "    model = SARIMAX(endog=train, exog=exog_df, order=order, seasonal_order=None)\n",
    "    fit_model = model.fit(disp=False)\n",
    "\n",
    "    # Make predictions\n",
    "    pred_list = fit_model.forecast(steps=len(test), exog=exog_test)\n",
    "    preds = pred_list[h-1]\n",
    "\n",
    "    # Calculate percentage and absolute errors\n",
    "    ar_perc_errors = np.abs(test[h-1]-preds)/test[h-1]\n",
    "    ar_abs_errors = np.abs(test[h-1]-preds)\n",
    "\n",
    "    # Store the percentage and absolute errors\n",
    "    ar_perc_error_list.append(ar_perc_errors)\n",
    "    ar_abs_error_list.append(ar_abs_errors)\n",
    "    \n",
    "    # Get the ARIMA predictions\n",
    "    ar_preds_train = np.append(ar_preds_train, preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2685fcd-e91f-4777-98cc-a89884759bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage-error results\n",
    "print('Mean absolute percentage error:', np.mean(ar_perc_error_list, axis = 0))\n",
    "print('Median absolute percentage error:', np.median(ar_perc_error_list, axis = 0))\n",
    "print('75th percentile of absolute percentage error:', np.percentile(ar_perc_error_list, 75, axis = 0))\n",
    "print('90th percentile of absolute percentage error:', np.percentile(ar_perc_error_list, 90, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b22e44-1f06-4702-992c-38068ae74298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the absolute error ratio results\n",
    "avg_global = atl_df['Total Volume'][T+h:].mean()\n",
    "print('Mean absolute error ratio:', np.mean(ar_abs_error_list, axis = 0)/avg_global)\n",
    "print('Median absolute error ratio:', np.median(ar_abs_error_list, axis = 0)/avg_global)\n",
    "print('75th percentile absolute error ratio:', np.percentile(ar_abs_error_list, 75, axis = 0)/avg_global)\n",
    "print('90th percentile absolute error ratio:', np.percentile(ar_abs_error_list, 90, axis = 0)/avg_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b93322-f716-4c06-a161-ef8663eee5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Atlanta data into train and test\n",
    "train_atl = atl_df[atl_df['Date'] <= '2016-11-27']\n",
    "test_atl = atl_df[atl_df['Date'] >= '2016-11-28']\n",
    "\n",
    "train_atl.head()\n",
    "train_atl.tail()\n",
    "\n",
    "test_atl.head()\n",
    "test_atl.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding lag features \n",
    "test_atl['lag1'] = test_atl['AveragePrice'].shift(periods = 1)\n",
    "test_atl['lag2'] = test_atl['AveragePrice'].shift(periods = 2)\n",
    "test_atl['lag3'] = test_atl['AveragePrice'].shift(periods = 3)\n",
    "test_atl['lag1tv'] = test_atl['Total Volume'].shift(periods = 1)\n",
    "test_atl['lag2tv'] = test_atl['Total Volume'].shift(periods = 2)\n",
    "test_atl['lag3tv'] = test_atl['Total Volume'].shift(periods = 3)\n",
    "test_atl.dropna(inplace = True) # This will drop the first three full weeks\n",
    "test_atl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea60128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding lag features \n",
    "train_atl['lag1'] = train_atl['AveragePrice'].shift(periods = 1)\n",
    "train_atl['lag2'] = train_atl['AveragePrice'].shift(periods = 2)\n",
    "train_atl['lag3'] = train_atl['AveragePrice'].shift(periods = 3)\n",
    "train_atl['lag1tv'] = train_atl['Total Volume'].shift(periods = 1)\n",
    "train_atl['lag2tv'] = train_atl['Total Volume'].shift(periods = 2)\n",
    "train_atl['lag3tv'] = train_atl['Total Volume'].shift(periods = 3)\n",
    "train_atl.dropna(inplace = True) # This will drop the first three full weeks\n",
    "train_atl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1137ab08-533b-4754-b0ee-a59be9dca244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding month feature\n",
    "test_atl['month'] = pd.to_datetime(test_atl['Date']).dt.month\n",
    "train_atl['month'] = pd.to_datetime(train_atl['Date']).dt.month\n",
    "train_atl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3188e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One shot training \n",
    "X_train = train_atl[['lag1', 'lag2', 'lag3', 'month', 'min_temp', 'max_temp', 'avg_temp', 'stdev_temp']]\n",
    "y_train = train_atl['AveragePrice']\n",
    "X_train.head()\n",
    "\n",
    "# defining the model and parameters\n",
    "gb = GradientBoostingRegressor(n_estimators = 100, max_depth = 5, min_samples_leaf = 2)\n",
    "\n",
    "# Asking the model to fit the training data\n",
    "gb = gb.fit(X_train, y_train) \n",
    "\n",
    "# Asking what the importance of features\n",
    "gb.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make forecasts using Gradient Boosting \n",
    "\n",
    "X_test = test_atl[['lag1', 'lag2', 'lag3', 'month', 'min_temp', 'max_temp', 'avg_temp', 'stdev_temp']]\n",
    "y_test = test_atl['AveragePrice']\n",
    "\n",
    "# Make predictions\n",
    "y_preds = gb.predict(X_test)\n",
    "\n",
    "# Calculate percentage and absolute errors\n",
    "perc_errors = np.abs(y_test-y_preds)/y_test\n",
    "abs_errors = np.abs(y_test-y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c815f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage-error results\n",
    "print('Mean absolute percentage error:', np.mean(perc_errors))\n",
    "print('Median absolute percentage error:', np.median(perc_errors))\n",
    "print('75th percentile of absolute percentage error:', np.percentile(perc_errors, 75))\n",
    "print('90th percentile of absolute percentage error:', np.percentile(perc_errors, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef641f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the absolute error ratio results\n",
    "avg_global = atl_df['AveragePrice'][T+h:].mean()\n",
    "print('Mean absolute error ratio:', np.mean(abs_errors)/avg_global)\n",
    "print('Median absolute error ratio:', np.median(abs_errors)/avg_global)\n",
    "print('75th percentile absolute error ratio:', np.percentile(abs_errors, 75)/avg_global)\n",
    "print('90th percentile absolute error ratio:', np.percentile(abs_errors, 90)/avg_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeee3c87-52e6-456f-8e04-bba9bcbd4565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3242711b",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc29fc6-a19d-4ea5-b90e-e6a2b6355197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One shot training based on previous year\n",
    "X_train = train_atl[['lag1tv', 'lag2tv', 'lag3tv', 'month', 'min_temp', 'max_temp', 'avg_temp', 'stdev_temp']]\n",
    "y_train = train_atl['Total Volume']\n",
    "X_train.head()\n",
    "\n",
    "# Make forecasts using Gradient Boosting for current year\n",
    "\n",
    "X_test = test_atl[['lag1tv', 'lag2tv', 'lag3tv', 'month', 'min_temp', 'max_temp', 'avg_temp', 'stdev_temp']]\n",
    "y_test = test_atl['Total Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBoost regressor with specific hyperparameters\n",
    "model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=1.0,\n",
    "    min_child_weight = 5.0, \n",
    "    colsample_bytree=1.0,\n",
    "    gamma = 5.0,\n",
    "    objective='reg:absoluteerror',\n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a197eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_preds = model.predict(X_test)\n",
    "# Calculate percentage and absolute errors\n",
    "perc_errors = np.abs(y_test-y_preds)/y_test\n",
    "abs_errors = np.abs(y_test-y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ed1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage-error results\n",
    "print('Mean absolute percentage error:', np.mean(perc_errors))\n",
    "print('Median absolute percentage error:', np.median(perc_errors))\n",
    "print('75th percentile of absolute percentage error:', np.percentile(perc_errors, 75))\n",
    "print('90th percentile of absolute percentage error:', np.percentile(perc_errors, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db1cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the absolute error ratio results\n",
    "avg_global = atl_df['Total Volume'][T+h:].mean()\n",
    "print('Mean absolute error ratio:', np.mean(abs_errors)/avg_global)\n",
    "print('Median absolute error ratio:', np.median(abs_errors)/avg_global)\n",
    "print('75th percentile absolute error ratio:', np.percentile(abs_errors, 75)/avg_global)\n",
    "print('90th percentile absolute error ratio:', np.percentile(abs_errors, 90)/avg_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2888826d",
   "metadata": {},
   "source": [
    "## XGB For Boston Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f20a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_df = final_df[final_df['region'] == 'Boston']\n",
    "hou_df = final_df[final_df['region'] == 'Houston']\n",
    "san_df = final_df[final_df['region'] == 'San Diego']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53afd378-501a-4186-a4fc-b87ae4b1f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_df = []\n",
    "hou_df.rename(columns={'Total Volume': 'Total Volume Hou'}, inplace = True)\n",
    "san_df.rename(columns={'Total Volume': 'Total Volume San'}, inplace = True)\n",
    "\n",
    "volume_df = bos_df.merge(hou_df[['Date', 'Total Volume Hou']], how = 'left', on = ['Date'])\n",
    "volume_df = volume_df.merge(san_df[['Date', 'Total Volume San']], how = 'left', on = ['Date'])\n",
    "volume_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322cb79-805f-492b-8ed4-5cbb0dcb5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_df['lag1tv'] = volume_df['Total Volume'].shift(periods = 1)\n",
    "volume_df['lag2tv'] = volume_df['Total Volume'].shift(periods = 2)\n",
    "volume_df['lag3tv'] = volume_df['Total Volume'].shift(periods = 3)\n",
    "volume_df['houtv'] = volume_df['Total Volume Hou'].shift(periods = 1)\n",
    "volume_df['santv'] = volume_df['Total Volume San'].shift(periods = 1)\n",
    "volume_df.dropna(inplace = True) # This will drop the first three full weeks\n",
    "volume_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e34fa-7f73-4424-9d24-47d0edb51815",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_df['month'] = pd.to_datetime(volume_df['Date']).dt.month\n",
    "train_bos = volume_df[volume_df['Date'] <= '2016-11-27']\n",
    "test_bos = volume_df[volume_df['Date'] >= '2016-11-28']\n",
    "\n",
    "# One shot training based on previous year\n",
    "X_train = train_bos[['lag1tv', 'lag2tv', 'lag3tv', 'month', 'min_temp', 'max_temp', 'avg_temp', 'stdev_temp']]\n",
    "y_train = train_bos['Total Volume']\n",
    "X_train.head()\n",
    "\n",
    "# Make forecasts using Gradient Boosting for current year\n",
    "\n",
    "X_test = test_bos[['lag1tv', 'lag2tv', 'lag3tv', 'month', 'min_temp', 'max_temp', 'avg_temp', 'stdev_temp']]\n",
    "y_test = test_bos['Total Volume']\n",
    "\n",
    "# Define the XGBoost regressor with specific hyperparameters\n",
    "model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=1.0,\n",
    "    min_child_weight = 5.0, \n",
    "    colsample_bytree=1.0,\n",
    "    gamma = 5.0,\n",
    "    objective='reg:absoluteerror',\n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_preds = model.predict(X_test)\n",
    "# Calculate percentage and absolute errors\n",
    "perc_errors = np.abs(y_test-y_preds)/y_test\n",
    "abs_errors = np.abs(y_test-y_preds)\n",
    "\n",
    "# Print the percentage-error results\n",
    "print('Mean absolute percentage error:', np.mean(perc_errors))\n",
    "print('Median absolute percentage error:', np.median(perc_errors))\n",
    "print('75th percentile of absolute percentage error:', np.percentile(perc_errors, 75))\n",
    "print('90th percentile of absolute percentage error:', np.percentile(perc_errors, 90))\n",
    "\n",
    "# Print the absolute error ratio results\n",
    "avg_global = atl_df['Total Volume'][T+h:].mean()\n",
    "print('Mean absolute error ratio:', np.mean(abs_errors)/avg_global)\n",
    "print('Median absolute error ratio:', np.median(abs_errors)/avg_global)\n",
    "print('75th percentile absolute error ratio:', np.percentile(abs_errors, 75)/avg_global)\n",
    "print('90th percentile absolute error ratio:', np.percentile(abs_errors, 90)/avg_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95722418-df62-4d05-8f2e-ea2ecbe24a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One shot training based on previous year\n",
    "X_train = train_bos[['lag1tv', 'lag2tv', 'lag3tv', 'month']]\n",
    "y_train = train_bos['Total Volume']\n",
    "X_train.head()\n",
    "\n",
    "# Make forecasts using Gradient Boosting for current year\n",
    "\n",
    "X_test = test_bos[['lag1tv', 'lag2tv', 'lag3tv', 'month']]\n",
    "y_test = test_bos['Total Volume']\n",
    "\n",
    "# Define the XGBoost regressor with specific hyperparameters\n",
    "model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=1.0,\n",
    "    min_child_weight = 5.0, \n",
    "    colsample_bytree=1.0,\n",
    "    gamma = 5.0,\n",
    "    objective='reg:absoluteerror',\n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_preds = model.predict(X_test)\n",
    "# Calculate percentage and absolute errors\n",
    "perc_errors = np.abs(y_test-y_preds)/y_test\n",
    "abs_errors = np.abs(y_test-y_preds)\n",
    "\n",
    "# Print the percentage-error results\n",
    "print('Mean absolute percentage error:', np.mean(perc_errors))\n",
    "print('Median absolute percentage error:', np.median(perc_errors))\n",
    "print('75th percentile of absolute percentage error:', np.percentile(perc_errors, 75))\n",
    "print('90th percentile of absolute percentage error:', np.percentile(perc_errors, 90))\n",
    "\n",
    "# Print the absolute error ratio results\n",
    "avg_global = atl_df['Total Volume'][T+h:].mean()\n",
    "print('Mean absolute error ratio:', np.mean(abs_errors)/avg_global)\n",
    "print('Median absolute error ratio:', np.median(abs_errors)/avg_global)\n",
    "print('75th percentile absolute error ratio:', np.percentile(abs_errors, 75)/avg_global)\n",
    "print('90th percentile absolute error ratio:', np.percentile(abs_errors, 90)/avg_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac007a35-fc81-486d-aea3-b236fd663e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One shot training based on previous year\n",
    "X_train = train_bos[['lag1tv', 'lag2tv', 'lag3tv', 'month', 'min_temp', 'max_temp', 'avg_temp', 'stdev_temp', 'houtv', 'santv']]\n",
    "y_train = train_bos['Total Volume']\n",
    "X_train.head()\n",
    "\n",
    "# Make forecasts using Gradient Boosting for current year\n",
    "\n",
    "X_test = test_bos[['lag1tv', 'lag2tv', 'lag3tv', 'month', 'min_temp', 'max_temp', 'avg_temp', 'stdev_temp', 'houtv', 'santv']]\n",
    "y_test = test_bos['Total Volume']\n",
    "\n",
    "# Define the XGBoost regressor with specific hyperparameters\n",
    "model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=1.0,\n",
    "    min_child_weight = 5.0, \n",
    "    colsample_bytree=1.0,\n",
    "    gamma = 5.0,\n",
    "    objective='reg:absoluteerror',\n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_preds = model.predict(X_test)\n",
    "# Calculate percentage and absolute errors\n",
    "perc_errors = np.abs(y_test-y_preds)/y_test\n",
    "abs_errors = np.abs(y_test-y_preds)\n",
    "\n",
    "# Print the percentage-error results\n",
    "print('Mean absolute percentage error:', np.mean(perc_errors))\n",
    "print('Median absolute percentage error:', np.median(perc_errors))\n",
    "print('75th percentile of absolute percentage error:', np.percentile(perc_errors, 75))\n",
    "print('90th percentile of absolute percentage error:', np.percentile(perc_errors, 90))\n",
    "\n",
    "# Print the absolute error ratio results\n",
    "avg_global = atl_df['Total Volume'][T+h:].mean()\n",
    "print('Mean absolute error ratio:', np.mean(abs_errors)/avg_global)\n",
    "print('Median absolute error ratio:', np.median(abs_errors)/avg_global)\n",
    "print('75th percentile absolute error ratio:', np.percentile(abs_errors, 75)/avg_global)\n",
    "print('90th percentile absolute error ratio:', np.percentile(abs_errors, 90)/avg_global)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
